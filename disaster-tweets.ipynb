{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428d318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76189c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training dataset with only the important colums\n",
    "train_dataset = pd.read_csv(\"nlp-getting-started/train.csv\", usecols=[3, 4])\n",
    "\n",
    "# Column 3 is the column of interest in the testing set\n",
    "test_dataset = pd.read_csv(\"nlp-getting-started/test.csv\", usecols=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dacdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print first 5 elements of the train dataset\n",
    "print(\"Training set: \", train_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print first 5 elements of the test dataset\n",
    "print(\"Testing set: \", test_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates in the dataset\n",
    "display(train_dataset.text.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce20b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the output we can see there are only 7503 unique values out of 7613 values,showing the dataset contains dulicates\n",
    "#Next we plot a pie chart to show the distribution of the dataset\n",
    "#We map the values 1 and 0 to disaster and not disaster respectively\n",
    "values = ['Disaster' if x == 1 else 'Not Disaster' for x in train_dataset.target.unique()]\n",
    "counts = train_dataset.target.value_counts()\n",
    "\n",
    "plt.pie(counts, labels=values, autopct='%1.1f%%')\n",
    "plt.title(\"The Number of Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903c8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see the data is somewhat evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be0ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the dataset\n",
    "\n",
    "#Removing duplicated text\n",
    "train_dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "#Function for cleaning text\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaned_train = train_dataset.copy()\n",
    "\n",
    "# Clean the 'text' column\n",
    "cleaned_train['text'] = cleaned_train['text'].apply(clean_text)\n",
    "\n",
    "# Print the cleaned dataset\n",
    "print(cleaned_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "null_values = cleaned_train.isnull().sum()\n",
    "\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot wordclouds for the different classes(disaster and non-disaster)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "disaster_words = WordCloud(\n",
    "    background_color='white',\n",
    "    width=800,\n",
    "    height=600\n",
    ").generate(\" \".join(cleaned_train[cleaned_train['target'] == 1]['text']))\n",
    "\n",
    "not_disaster_words = WordCloud(\n",
    "    background_color='white',\n",
    "    width=800,\n",
    "    height=600\n",
    ").generate(\" \".join(cleaned_train[cleaned_train['target'] == 0]['text']))\n",
    "\n",
    "ax[0].imshow(disaster_words, interpolation='bilinear')\n",
    "ax[0].set_title('\\nDisaster Words\\n', fontsize=16)\n",
    "ax[0].axis('off')\n",
    "print('\\n')\n",
    "ax[1].imshow(not_disaster_words, interpolation='bilinear')\n",
    "ax[1].set_title('\\nNot Disaster Words\\n', fontsize=16)\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ce4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing data for traing on our models\n",
    "\n",
    "#Spliting data into input features x and y\n",
    "X = cleaned_train['text'].values\n",
    "y = cleaned_train['target'].values\n",
    "\n",
    "#Splitting dataset into test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Transforming the training data using a CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Using a TfidfTransformer to transform the count vectors\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Transforming the test data\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f458455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 3 different models on the  preprocessed dataset \n",
    "# and compare their metrics,to get the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Multinomial Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Linear Support Vector Classifier (SVC) model\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(X_train_tfidf, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "logreg_pred = logreg_model.predict(X_test_tfidf)\n",
    "nb_pred = nb_model.predict(X_test_tfidf)\n",
    "svc_pred = svc_model.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56763a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the metrics of the models\n",
    "def print_metrics(y_true, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "print_metrics(y_test, logreg_pred, \"Logistic Regression\")\n",
    "print_metrics(y_test, nb_pred, \"Multinomial Naive Bayes\")\n",
    "print_metrics(y_test, svc_pred, \"Linear SVC\")\n",
    "\n",
    "print_metrics(y_test, logreg_pred, \"Logistic Regression\")\n",
    "print_metrics(y_test, nb_pred, \"Multinomial Naive Bayes\")\n",
    "print_metrics(y_test, svc_pred, \"Linear SVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58f50eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected model to classify the data in the test \n",
    "# dataset and map it to a .csv file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test dataset\n",
    "print(test_dataset.keys())\n",
    "test_dataset_cleaned = test_dataset.copy()\n",
    "test_dataset_cleaned['text'] = test_dataset_cleaned['text'].apply(\n",
    "    clean_text)\n",
    "X_submission = test_dataset_cleaned['text'].values\n",
    "X_submission_counts = count_vectorizer.transform(X_submission)\n",
    "X_submission_tfidf = tfidf_transformer.transform(X_submission_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the submission dataset\n",
    "def model_classification(model, model_name=\"model\"):\n",
    "    submission_pred = model.predict(X_submission_tfidf)\n",
    "\n",
    "    # Create a submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_dataset['id'],\n",
    "        'target': submission_pred\n",
    "    })\n",
    "\n",
    "    # Save the submission DataFrame to a .csv file\n",
    "    submission_df.to_csv(\"nlp-getting-started/\" +\n",
    "                         model_name + \"_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected model for classification\n",
    "# Logistic Regression model\n",
    "model_classification(logreg_model, \"logistic_regression_model\")\n",
    "\n",
    "# Multinomial Naive Bayes model\n",
    "model_classification(nb_model, \"multinomial_naive_bayes_model\")\n",
    "\n",
    "# Linear Support Vector Classifier (SVC) model\n",
    "model_classification(svc_model, \"linear_support_vector_classifier_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
